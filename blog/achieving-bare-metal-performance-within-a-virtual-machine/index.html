<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://www.cloudhypervisor.org/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://www.cloudhypervisor.org/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var b=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,a=localStorage.getItem("theme");b&&a===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),b&&a==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),a==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=https://www.cloudhypervisor.org/main.1c0e5007559e4bba3b6514140ab7970c8b72eb19070220671efd6eaa3ee1aaaaf02390c5c0addbd11a3eb4259bbbf1ac5bf49cc3c4b5b58c58a3071071c80988.css integrity="sha512-HA5QB1WeS7o7ZRQUCreXDIty6xkHAiBnHv1uqj7hqqrwI5DFwK3b0Ro+tCWbu/GsW/Scw8S1tYxYowcQccgJiA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Achieving Bare Metal Performance Within a Virtual Machine - Cloud Hypervisor</title><meta name=description content="There are use cases for which a workload needs to access specific hardware such as accelerators, GPU or network adapters to maximise potential performance. And since these workloads run inside virtual machines (VM) for security reasons, the challenge is to make this hardware available from within the VM, but without degrading the performance that can be achieved from bare metal.
VFIO, the Ideal Choice # This is a mature framework allowing a PCI device to be bound to the vfio-pci driver instead of the default driver it is usually attached to."><link rel=canonical href=https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Achieving Bare Metal Performance Within a Virtual Machine"><meta property="og:description" content="There are use cases for which a workload needs to access specific hardware such as accelerators, GPU or network adapters to maximise potential performance. And since these workloads run inside virtual machines (VM) for security reasons, the challenge is to make this hardware available from within the VM, but without degrading the performance that can be achieved from bare metal.
VFIO, the Ideal Choice # This is a mature framework allowing a PCI device to be bound to the vfio-pci driver instead of the default driver it is usually attached to."><meta property="og:url" content="https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/"><meta property="og:site_name" content="Cloud Hypervisor"><meta property="article:published_time" content="2022-05-19T08:00:00+00:00"><meta property="article:modified_time" content="2022-05-19T08:00:00+00:00"><meta property="og:image" content="https://www.cloudhypervisor.org/doks.png"><meta property="og:image:alt" content="Cloud Hypervisor"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.cloudhypervisor.org/#/schema/organization/1","name":"Cloud Hypervisor","url":"https://www.cloudhypervisor.org/","sameAs":["https://github.com/cloud-hypervisor/cloud-hypervisor"],"logo":{"@type":"ImageObject","@id":"https://www.cloudhypervisor.org/#/schema/image/1","url":"https://www.cloudhypervisor.org/\u003cnil\u003e","width":null,"height":null,"caption":"Cloud Hypervisor"},"image":{"@id":"https://www.cloudhypervisor.org/#/schema/image/1"}},{"@type":"WebSite","@id":"https://www.cloudhypervisor.org/#/schema/website/1","url":"https://www.cloudhypervisor.org/","name":"Cloud Hypervisor","description":"Cloud Hypervisor is an open source Virtual Machine Monitor (VMM) implemented in Rust that focuses on running modern, cloud workloads, with minimal hardware emulation.","publisher":{"@id":"https://www.cloudhypervisor.org/#/schema/organization/1"}},{"@type":"WebPage","@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/","url":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/","name":"Achieving Bare Metal Performance Within a Virtual Machine","description":"","isPartOf":{"@id":"https://www.cloudhypervisor.org/#/schema/website/1"},"about":{"@id":"https://www.cloudhypervisor.org/#/schema/organization/1"},"datePublished":"2022-05-19T08:00:00CET","dateModified":"2022-05-19T08:00:00CET","breadcrumb":{"@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/"]}]},{"@type":"BreadcrumbList","@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://www.cloudhypervisor.org","url":"https://www.cloudhypervisor.org","name":"Home"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://www.cloudhypervisor.org/blog/","url":"https://www.cloudhypervisor.org/blog/","name":"Blog"}},{"@type":"ListItem","position":4,"item":{"@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://www.cloudhypervisor.org/#/schema/article/1","headline":"Achieving Bare Metal Performance Within a Virtual Machine","description":"","isPartOf":{"@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/"},"mainEntityOfPage":{"@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/"},"datePublished":"2022-05-19T08:00:00CET","dateModified":"2022-05-19T08:00:00CET","author":{"@id":"https://www.cloudhypervisor.org/#/schema/person/2"},"publisher":{"@id":"https://www.cloudhypervisor.org/#/schema/organization/1"},"image":{"@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://www.cloudhypervisor.org/#/schema/person/2","name":null,"sameAs":[]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://www.cloudhypervisor.org/blog/achieving-bare-metal-performance-within-a-virtual-machine/#/schema/image/2","url":"https://www.cloudhypervisor.org/doks.png","contentUrl":"https://www.cloudhypervisor.org/doks.png","caption":"Achieving Bare Metal Performance Within a Virtual Machine"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://www.cloudhypervisor.org/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://www.cloudhypervisor.org/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://www.cloudhypervisor.org/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://www.cloudhypervisor.org/site.webmanifest></head><body class="blog single"><div class=header-bar></div><header class="navbar navbar-expand-md navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-md-nowrap" aria-label="Main navigation"><a class="navbar-brand p-0 me-auto" href=https://www.cloudhypervisor.org/ aria-label="Cloud Hypervisor">Cloud Hypervisor</a>
<button class="btn btn-menu d-block d-md-none order-5" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-start border-0 py-md-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-md-none"></div><div class="offcanvas-header d-md-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=https://www.cloudhypervisor.org/>Cloud Hypervisor</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body px-4"><h3 class="h6 text-uppercase mb-3 d-md-none">Main</h3><ul class="nav flex-column flex-md-row ms-md-n3"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://www.cloudhypervisor.org/docs/prologue/introduction/>Docs</a></li><li class=nav-item><a class="nav-link ps-0 py-1 active" href=https://www.cloudhypervisor.org/blog/>Blog</a></li></ul><hr class="text-black-50 my-4 d-md-none"><h3 class="h6 text-uppercase mb-3 d-md-none">Socials</h3><ul class="nav flex-column flex-md-row ms-md-auto me-md-n5 pe-md-2"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/cloud-hypervisor/cloud-hypervisor><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-md-none">GitHub</small></a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://join.slack.com/t/cloud-hypervisor/shared_invite/enQtNjY3MTE3MDkwNDQ4LWQ1MTA1ZDVmODkwMWQ1MTRhYzk4ZGNlN2UwNTI3ZmFlODU0OTcwOWZjMTkwZDExYWE3YjFmNzgzY2FmNDAyMjI><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-slack"><path d="M14.5 10c-.83.0-1.5-.67-1.5-1.5v-5c0-.83.67-1.5 1.5-1.5s1.5.67 1.5 1.5v5c0 .83-.67 1.5-1.5 1.5z"/><path d="M20.5 10H19V8.5c0-.83.67-1.5 1.5-1.5s1.5.67 1.5 1.5-.67 1.5-1.5 1.5z"/><path d="M9.5 14c.83.0 1.5.67 1.5 1.5v5c0 .83-.67 1.5-1.5 1.5S8 21.33 8 20.5v-5c0-.83.67-1.5 1.5-1.5z"/><path d="M3.5 14H5v1.5c0 .83-.67 1.5-1.5 1.5S2 16.33 2 15.5 2.67 14 3.5 14z"/><path d="M14 14.5c0-.83.67-1.5 1.5-1.5h5c.83.0 1.5.67 1.5 1.5s-.67 1.5-1.5 1.5h-5c-.83.0-1.5-.67-1.5-1.5z"/><path d="M15.5 19H14v1.5c0 .83.67 1.5 1.5 1.5s1.5-.67 1.5-1.5-.67-1.5-1.5-1.5z"/><path d="M10 9.5C10 8.67 9.33 8 8.5 8h-5C2.67 8 2 8.67 2 9.5S2.67 11 3.5 11h5c.83.0 1.5-.67 1.5-1.5z"/><path d="M8.5 5H10V3.5C10 2.67 9.33 2 8.5 2S7 2.67 7 3.5 7.67 5 8.5 5z"/></svg><small class="ms-2 d-md-none">Slack</small></a></li></ul></div></div><button id=mode class="btn btn-link order-md-1" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></nav></header><div class="wrap container-xxl" role=document><div class=content><div class="row justify-content-center"><div class="col-md-12 col-lg-10 col-xl-8"><article><div class=blog-header><h1>Achieving Bare Metal Performance Within a Virtual Machine</h1><p><small>Posted May 19, 2022 by <a class="stretched-link position-relative" href=https://www.cloudhypervisor.org/contributors/sebastien-boeuf/>Sebastien Boeuf</a>&nbsp;&dash;&nbsp;<strong>7&nbsp;min read</strong></small><p></div><p class=lead></p><p>There are use cases for which a workload needs to access specific hardware such
as accelerators, GPU or network adapters to maximise potential performance. And
since these workloads run inside virtual machines (VM) for security reasons, the
challenge is to make this hardware available from within the VM, but without
degrading the performance that can be achieved from bare metal.</p><h3 id=vfio-the-ideal-choice>VFIO, the Ideal Choice <a href=#vfio-the-ideal-choice class=anchor aria-hidden=true>#</a></h3><p>This is a mature framework allowing a PCI device to be bound to the <code>vfio-pci</code>
driver instead of the default driver it is usually attached to.</p><p>It exposes an ioctl interface so that a userspace program can interact with the
device and retrieve all the information it needs. In the virtualization case,
this userspace program is the Virtual Machine Monitor (VMM), Cloud Hypervisor
for instance, which after it got all the information from the device, exposes
them to the guest. And that is how the workload can access the device inside the
VM, exactly the same way it would if it was directly running on bare metal.</p><p>Now that we&rsquo;ve covered the functional aspect, let&rsquo;s have a look at the way we
can achieve bare metal performance from the guest.</p><p>The main idea for not degrading performance is to avoid as much as possible to
trigger VM exits when the workload interacts with the device, as well as when
it receives interrupts from it.</p><h4 id=base-address-registers>Base Address Registers <a href=#base-address-registers class=anchor aria-hidden=true>#</a></h4><p>First, we can look at the PCI Base Address Registers (BAR). We need to prevent
a BAR access from generating a VM exit. This is done by mapping the device
region directly into the VMM virtual address space, and make this same region a
user memory region at the hypervisor level. This means we tell KVM to inform
the CPU it can find the pages related to this region directly at a specific
address in physical RAM. Once this is properly configured, any memory access
from guest workload to one of the device&rsquo;s PCI BARs will result in a direct
access to physical RAM from the CPU as it can find the pages from the Extended
Page Table (EPT).</p><p>There&rsquo;s one small part of the BARs for which we can&rsquo;t avoid VM exits, the MSI-X
tables. MSI-X specification expects the vector table and pending bit array to be
part of the device BARs. But the VMM needs to trap every access to these tables
since it&rsquo;s responsible for configuring the interrupts both at the KVM and VFIO
levels. This is important to note this doesn&rsquo;t impact the performance of the
device when being used by the guest. The interrupt vectors are only configured
during the initialization of the device driver, which is already completed at
the time the workload interacts with the device.</p><h4 id=direct-memory-accesses>Direct Memory Accesses <a href=#direct-memory-accesses class=anchor aria-hidden=true>#</a></h4><p>The second aspect, the main one actually, is how Direct Memory Access (DMA) is
performed securely without generating any VM exit. DMA is the standard way for
a device to access physical RAM without involving CPU cycles, which allows the
device to achieve very high performance. In the context of virtualization, we
want to reuse this mechanism, but without allowing a malicious guest to use this
device to reach every address from physical RAM. That&rsquo;s where the IOMMU comes
into play, as the gatekeeper of RAM accesses from any device on the machine.
The VMM is responsible for configuring the IOMMU through the VFIO interface. It
provides the set of pages that can be accessed from a specific device, which
usually means the entire guest RAM. This information is stored in the DMA
Remapping (DMAR) table as part of the IOMMU. Whenever the workload initiate a
DMA transfer between the device and the guest RAM, the IOMMU will allow such
access, but if the address is outside the authorized range, the IOMMU will
prevent the transfer from happening. That&rsquo;s how security is enforced in the
virtualization context.</p><p>Since DMA doesn&rsquo;t involve CPU to access the memory, a DMA transfer doesn&rsquo;t
trigger any VM exit, providing identical performance to what can be observed
on bare metal.</p><h4 id=interrupts>Interrupts <a href=#interrupts class=anchor aria-hidden=true>#</a></h4><p>Third and last aspect, the interrupts. Depending on the type of device, we can
see a lot of interrupts being generated, which can trigger a large amount of VM
exits, affecting directly the performance of the device inside the VM.</p><p>The way to avoid such disruption is by relying on fairly recent hardware to
leverage a feature called Posted Interrupts (PI). This is both part of the CPU
with the virtual APIC (APICv) and the IOMMU through the Interrupt Remapping (IR)
table. When the hardware supports it, the hypervisor on behalf of the VMM will
configure the IOMMU by adding new entries to the IR table. Later on, whenever
an interrupt is triggered, the IOMMU will check the IR table to find out the PI
descriptor associated with it. It then triggers the correct interrupt inside the
guest relying on the APICv. This whole chain of events is entirely handled in
hardware, meaning there&rsquo;s no VM exit involved, which doesn&rsquo;t lead to any drop
in performance.</p><h4 id=how-to-use-vfio>How To Use VFIO <a href=#how-to-use-vfio class=anchor aria-hidden=true>#</a></h4><p>For more details on how to use VFIO with Cloud Hypervisor, refer to the
following <a href=https://github.com/cloud-hypervisor/cloud-hypervisor/blob/main/docs/vfio.md>documentation</a>.</p><h3 id=vdpa-an-interesting-alternative>vDPA, an Interesting Alternative <a href=#vdpa-an-interesting-alternative class=anchor aria-hidden=true>#</a></h3><p>vDPA stands for virtio Data Path Acceleration, and the main reason this new
framework exists is its ability to simplify the migration process. The whole
concept behind vDPA relies on the virtio specification, which is what makes it
more suited for migration.</p><h4 id=control-path>Control Path <a href=#control-path class=anchor aria-hidden=true>#</a></h4><p>On the one hand, the virtio control path is used for configuring the virtqueues
and getting information about the device. This is achieved through the ioctl
interface that is accessible through the device node <code>/dev/vhost-vdpa</code>. The
userspace program, the VMM again, retrieves all the information it needs so it
can expose the correct type of virtio device to the guest, with the right amount
of queues, the size of each queue, and the virtio configuration associated with.</p><h4 id=data-path>Data Path <a href=#data-path class=anchor aria-hidden=true>#</a></h4><p>On the other hand, the virtio data path, effectively the virtqueues themselves,
is used to transfer the data between frontend in the guest and backend from the
host. But in this very special case, the virtqueues are directly part of the
physical device, allowing DMA transfers to be initiated by the guest. And the
same way it works with VFIO because the device is also attached to a physical
IOMMU, the guest can perform secure and efficient memory accesses from the
device.</p><h4 id=migration-experimental>Migration (experimental) <a href=#migration-experimental class=anchor aria-hidden=true>#</a></h4><p>Providing a simpler path for migrating physical devices from one VM to another
is one of the main reason vDPA was created. Because of the design relying on
virtqueues, most of the migration complexity can be handled by the VMM, without
the need for the vDPA device to provide an opaque blob of data that will have to
be restored on the destination VM. That&rsquo;s where it differentiates from the VFIO
approach, which will require every vendor to implement the VFIO migration API as
part of their driver, providing an opaque blob specific to each device.</p><p>It&rsquo;s important to note that VFIO migration API is very recent and not heavily
tested and deployed, meaning you might want to wait for a bit before jumping
to it.</p><p>For vDPA, which is a very recent addition to the Linux kernel, the migration
isn&rsquo;t fully figured out yet by the maintainers since it still requires some
extra support through the virtio specification that will allow to stop a device
before it can be migrated.
There&rsquo;s an alternative approach implemented in software part of the Data Plan
Development Kit (DPDK). It shadows the virtqueues to have complete knowledge of
what is happening inside the queues, and therefore at any point in time it can
stop the device and start migrating.</p><h4 id=how-to-use-vdpa>How To Use vDPA <a href=#how-to-use-vdpa class=anchor aria-hidden=true>#</a></h4><p>For more details on how to use vDPA with Cloud Hypervisor, refer to the
following <a href=https://github.com/cloud-hypervisor/cloud-hypervisor/blob/main/docs/vdpa.md>documentation</a>.</p></article></div></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright © 2021 Cloud Hypervisor a Series of LF Projects, LLC. For website terms of use, trademark policy, privacy policy and other project policies please see https://lfprojects.org/policies. Rendered with <a href=https://gohugo.io/>Hugo</a>, and <a href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=https://www.cloudhypervisor.org/js/bootstrap.min.95ff02c70fdf8d74cbaff489392892a6de80f38df5e2752e0bb428a7cccbe3dbfa601559ef22476f745d08f6b5f8ae05f7cfced9aaffb6bdcdf4ad4d53ec88d5.js integrity="sha512-lf8Cxw/fjXTLr/SJOSiSpt6A84314nUuC7Qop8zL49v6YBVZ7yJHb3RdCPa1+K4F98/O2ar/tr3N9K1NU+yI1Q==" crossorigin=anonymous defer></script>
<script src=https://www.cloudhypervisor.org/js/highlight.min.7ed281d7c8c320aa65d428e365ed0205692ea52a681b20a1eabb7fb1832629449ac1fecc31bab7e0f6755f3706adea74583a6208f8ab5ef3ff5c83d36cd710e4.js integrity="sha512-ftKB18jDIKpl1CjjZe0CBWkupSpoGyCh6rt/sYMmKUSawf7MMbq34PZ1XzcGrep0WDpiCPirXvP/XIPTbNcQ5A==" crossorigin=anonymous defer></script>
<script src=https://www.cloudhypervisor.org/main.min.abc62e0b94c4425ee84af3577589f5e1e4ce00c3772b81ea9b109b8a12c5c92391766996cf215c6bbdac7f99ec495bee6655124c77b387f58eb5a825a6a0a3d7.js integrity="sha512-q8YuC5TEQl7oSvNXdYn14eTOAMN3K4HqmxCbihLFySORdmmWzyFca72sf5nsSVvuZlUSTHezh/WOtaglpqCj1w==" crossorigin=anonymous defer></script></body></html>