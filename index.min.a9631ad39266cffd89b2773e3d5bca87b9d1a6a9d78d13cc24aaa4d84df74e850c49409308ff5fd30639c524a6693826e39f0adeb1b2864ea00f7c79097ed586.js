var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(c){const d=suggestions.classList.contains('d-none');if(d)return;const a=[...suggestions.querySelectorAll('a')];if(a.length===0)return;const b=a.indexOf(document.activeElement);if(c.key==="ArrowUp"){c.preventDefault();const d=b>0?b-1:0;a[d].focus()}else if(c.key==="ArrowDown"){c.preventDefault();const d=b+1<a.length?b+1:b;a[d].focus()}}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/prologue/introduction/",title:"Introduction",description:"Cloud Hypervisor is an open source Virtual Machine Monitor (VMM) implemented in Rust that focuses on running modern, cloud workloads, with minimal hardware emulation.",content:"Quick Start # See the Quick Start â†’ for a single page guide to getting started with Cloud Hypervisor.\nGo further # Currently the majority of the documentation is in the source tree but a user guide, which will be hosted on this website is planned.\nUser documentation # Here are some documents related to particular features and functionality that a user of Cloud Hypervisor might be interested in:\n HTTP API Virtual CPU configuration Using virtio-fs How to hotplug devices How to use Intel SGX vIOMMU configuration I/O throttling for network and block devices Live migration Using a MACvTAP bridge to place a VM on the host network Advanced memory configuration Snapshot \u0026amp; Restore of a VM Building a UEFI firmware for use with Cloud Hypervisor Using VFIO to passthrough PCI devices Emulated NVMe controller via vfio-user Using virtio-fs as as a root filesystem Running and debugging Windows guests  Developer documentation # These documents are primarily for developers who are working on the project:\n Updating the custom image Debug port logging for performance testing Correct use of logging How to profile Cloud Hypervisor Seccomp filter debugging Testing vhost-user-block Testing vhost-user-net  Contributing # See the CONTRIBUTING file in the source tree for details of how to contribute.\nHelp # Need help with Cloud Hypervisor? Open a discussion thread on Github or ask in our Slack.\n"}).add({id:1,href:"/docs/prologue/quick-start/",title:"Quick Start",description:"One page summary of how to get started with Cloud Hypervisor",content:"The following sections describe how to build and run Cloud Hypervisor.\nPrerequisites for AArch64 #  AArch64 servers (recommended) or development boards equipped with the GICv3 interrupt controller.  Host OS # For required KVM functionality the minimum host kernel version is 4.11. For adequate performance the minimum recommended host kernel version is 5.6. The majority of the CI currently tests with kernel version 5.15.\nUse Pre-built Binaries # The recommended approach to getting started with Cloud Hypervisor is by using a pre-built binary. Binaries are available for the latest release. Use cloud-hypervisor-static for x86-64 or cloud-hypervisor-static-aarch64 for AArch64 platform.\nPackages # For convenience, packages are also available targeting some popular Linux distributions. This is thanks to the Open Build Service. The OBS README explains how to enable the repository in a supported Linux distribution and install Cloud Hypervisor and accompanying packages. Please report any packaging issues in the obs-packaging repository.\nBuilding from Source # Please see the instructions for building from source if you do not wish to use the pre-built binaries.\nBooting Linux # Cloud Hypervisor supports direct kernel boot (the x86-64 kernel requires the kernel built with PVH support) or booting via a firmware (either Rust Hypervisor Firmware or an edk2 UEFI firmware called CLOUDHV / CLOUDHV_EFI.)\nBinary builds of the firmware files are available for the latest release of Rust Hypervisor Firmware and our edk2 repository\nThe choice of firmware depends on your guest OS choice; some experimentation may be required.\nFirmware Booting # Cloud Hypervisor supports booting disk images containing all needed components to run cloud workloads, a.k.a. cloud images.\nThe following sample commands will download an Ubuntu Cloud image, converting it into a format that Cloud Hypervisor can use and a firmware to boot the image with.\n$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img $ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw $ wget https://github.com/cloud-hypervisor/rust-hypervisor-firmware/releases/download/0.4.2/hypervisor-fw  The Ubuntu cloud images do not ship with a default password so it necessary to use a cloud-init disk image to customise the image on the first boot. A basic cloud-init image is generated by this script. This seeds the image with a default username/password of cloud/cloud123. It is only necessary to add this disk image on the first boot. Script also assigns default IP address using test_data/cloud-init/ubuntu/local/network-config details with --net \u0026quot;mac=12:34:56:78:90:ab,tap=\u0026quot; option. Then the matching mac address interface will be enabled as per network-config details.\n$ sudo setcap cap_net_admin+ep ./cloud-hypervisor $ ./create-cloud-init.sh $ ./cloud-hypervisor \\ --kernel ./hypervisor-fw \\ --disk path=focal-server-cloudimg-amd64.raw path=/tmp/ubuntu-cloudinit.img \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;  If access to the firmware messages or interaction with the boot loader (e.g. GRUB) is required then it necessary to switch to the serial console instead of virtio-console.\n$ ./cloud-hypervisor \\ --kernel ./hypervisor-fw \\ --disk path=focal-server-cloudimg-amd64.raw path=/tmp/ubuntu-cloudinit.img \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot; \\ --serial tty \\ --console off  Custom Kernel and Disk Image # Building your Kernel # Cloud Hypervisor also supports direct kernel boot. For x86-64, a vmlinux ELF kernel (compiled with PVH support) is needed. In order to support development there is a custom branch; however provided the required options are enabled any recent kernel will suffice.\nTo build the kernel:\n# Clone the Cloud Hypervisor Linux branch $ git clone --depth 1 https://github.com/cloud-hypervisor/linux.git -b ch-6.2 linux-cloud-hypervisor $ pushd linux-cloud-hypervisor # Use the x86-64 cloud-hypervisor kernel config to build your kernel for x86-64 $ wget https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/resources/linux-config-x86_64 # Use the AArch64 cloud-hypervisor kernel config to build your kernel for AArch64 $ wget https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/resources/linux-config-aarch64 $ cp linux-config-x86_64 .config # x86-64 $ cp linux-config-aarch64 .config # AArch64 # Do native build of the x86-64 kernel $ KCFLAGS=\u0026quot;-Wa,-mx86-used-note=no\u0026quot; make bzImage -j `nproc` # Do native build of the AArch64 kernel $ make -j `nproc` $ popd  For x86-64, the vmlinux kernel image will then be located at linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin. For AArch64, the Image kernel image will then be located at linux-cloud-hypervisor/arch/arm64/boot/Image.\nDisk image # For the disk image the same Ubuntu image as before can be used. This contains an ext4 root filesystem.\n$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img # x86-64 $ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-arm64.img # AArch64 $ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw # x86-64 $ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-arm64.img focal-server-cloudimg-arm64.raw # AArch64  Booting the guest VM # These sample commands boot the disk image using the custom kernel whilst also supplying the desired kernel command line.\n x86-64  $ sudo setcap cap_net_admin+ep ./cloud-hypervisor $ ./create-cloud-init.sh $ ./cloud-hypervisor \\ --kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \\ --disk path=focal-server-cloudimg-amd64.raw path=/tmp/ubuntu-cloudinit.img \\ --cmdline \u0026quot;console=hvc0 root=/dev/vda1 rw\u0026quot; \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;   AArch64  $ sudo setcap cap_net_admin+ep ./cloud-hypervisor $ ./create-cloud-init.sh $ ./cloud-hypervisor \\ --kernel ./linux-cloud-hypervisor/arch/arm64/boot/Image \\ --disk path=focal-server-cloudimg-arm64.raw path=/tmp/ubuntu-cloudinit.img \\ --cmdline \u0026quot;console=hvc0 root=/dev/vda1 rw\u0026quot; \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;  If earlier kernel messages are required the serial console should be used instead of virtio-console.\n x86-64  $ ./cloud-hypervisor \\ --kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \\ --console off \\ --serial tty \\ --disk path=focal-server-cloudimg-amd64.raw \\ --cmdline \u0026quot;console=ttyS0 root=/dev/vda1 rw\u0026quot; \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;   AArch64  $ ./cloud-hypervisor \\ --kernel ./linux-cloud-hypervisor/arch/arm64/boot/Image \\ --console off \\ --serial tty \\ --disk path=focal-server-cloudimg-arm64.raw \\ --cmdline \u0026quot;console=ttyAMA0 root=/dev/vda1 rw\u0026quot; \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;  "}).add({id:2,href:"/docs/prologue/commands/",title:"Commands",description:"Cloud Hypervisor is composed of multiple binaries. This document shows what they are and their help output.",content:"cloud-hypervisor binary # The main cloud-hypervisor has the following help output (as of v37.0):\ncloud-hypervisor --help Launch a cloud-hypervisor VMM. Usage: cloud-hypervisor [OPTIONS] Options: --cpus \u0026lt;cpus\u0026gt; boot=\u0026lt;boot_vcpus\u0026gt;,max=\u0026lt;max_vcpus\u0026gt;,topology=\u0026lt;threads_per_core\u0026gt;:\u0026lt;cores_per_die\u0026gt;:\u0026lt;dies_per_package\u0026gt;:\u0026lt;packages\u0026gt;,kvm_hyperv=on|off,max_phys_bits=\u0026lt;maximum_number_of_physical_bits\u0026gt;,affinity=\u0026lt;list_of_vcpus_with_their_associated_cpuset\u0026gt;,features=\u0026lt;list_of_features_to_enable\u0026gt; [default: boot=1,max_phys_bits=46] --platform \u0026lt;platform\u0026gt; num_pci_segments=\u0026lt;num_pci_segments\u0026gt;,iommu_segments=\u0026lt;list_of_segments\u0026gt;,serial_number=\u0026lt;dmi_device_serial_number\u0026gt;,uuid=\u0026lt;dmi_device_uuid\u0026gt;,oem_strings=\u0026lt;list_of_strings\u0026gt; --memory \u0026lt;memory\u0026gt; Memory parameters \u0026quot;size=\u0026lt;guest_memory_size\u0026gt;,mergeable=on|off,shared=on|off,hugepages=on|off,hugepage_size=\u0026lt;hugepage_size\u0026gt;,hotplug_method=acpi|virtio-mem,hotplug_size=\u0026lt;hotpluggable_memory_size\u0026gt;,hotplugged_size=\u0026lt;hotplugged_memory_size\u0026gt;,prefault=on|off,thp=on|off\u0026quot; [default: size=512M] --memory-zone \u0026lt;memory-zone\u0026gt;... User defined memory zone parameters \u0026quot;size=\u0026lt;guest_memory_region_size\u0026gt;,file=\u0026lt;backing_file\u0026gt;,shared=on|off,hugepages=on|off,hugepage_size=\u0026lt;hugepage_size\u0026gt;,host_numa_node=\u0026lt;node_id\u0026gt;,id=\u0026lt;zone_identifier\u0026gt;,hotplug_size=\u0026lt;hotpluggable_memory_size\u0026gt;,hotplugged_size=\u0026lt;hotplugged_memory_size\u0026gt;,prefault=on|off\u0026quot; --firmware \u0026lt;firmware\u0026gt; Path to firmware that is loaded in an architectural specific way --kernel \u0026lt;kernel\u0026gt; Path to kernel to load. This may be a kernel or firmware that supports a PVH entry point (e.g. vmlinux) or architecture equivalent --initramfs \u0026lt;initramfs\u0026gt; Path to initramfs image --cmdline \u0026lt;cmdline\u0026gt; Kernel command line --disk \u0026lt;disk\u0026gt;... Disk parameters \u0026quot;path=\u0026lt;disk_image_path\u0026gt;,readonly=on|off,direct=on|off,iommu=on|off,num_queues=\u0026lt;number_of_queues\u0026gt;,queue_size=\u0026lt;size_of_each_queue\u0026gt;,vhost_user=on|off,socket=\u0026lt;vhost_user_socket_path\u0026gt;,bw_size=\u0026lt;bytes\u0026gt;,bw_one_time_burst=\u0026lt;bytes\u0026gt;,bw_refill_time=\u0026lt;ms\u0026gt;,ops_size=\u0026lt;io_ops\u0026gt;,ops_one_time_burst=\u0026lt;io_ops\u0026gt;,ops_refill_time=\u0026lt;ms\u0026gt;,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --net \u0026lt;net\u0026gt;... Network parameters \u0026quot;tap=\u0026lt;if_name\u0026gt;,ip=\u0026lt;ip_addr\u0026gt;,mask=\u0026lt;net_mask\u0026gt;,mac=\u0026lt;mac_addr\u0026gt;,fd=\u0026lt;fd1,fd2...\u0026gt;,iommu=on|off,num_queues=\u0026lt;number_of_queues\u0026gt;,queue_size=\u0026lt;size_of_each_queue\u0026gt;,id=\u0026lt;device_id\u0026gt;,vhost_user=\u0026lt;vhost_user_enable\u0026gt;,socket=\u0026lt;vhost_user_socket_path\u0026gt;,vhost_mode=client|server,bw_size=\u0026lt;bytes\u0026gt;,bw_one_time_burst=\u0026lt;bytes\u0026gt;,bw_refill_time=\u0026lt;ms\u0026gt;,ops_size=\u0026lt;io_ops\u0026gt;,ops_one_time_burst=\u0026lt;io_ops\u0026gt;,ops_refill_time=\u0026lt;ms\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;offload_tso=on|off,offload_ufo=on|off,offload_csum=on|off\u0026quot; --rng \u0026lt;rng\u0026gt; Random number generator parameters \u0026quot;src=\u0026lt;entropy_source_path\u0026gt;,iommu=on|off\u0026quot; [default: src=/dev/urandom] --balloon \u0026lt;balloon\u0026gt; Balloon parameters \u0026quot;size=\u0026lt;balloon_size\u0026gt;,deflate_on_oom=on|off,free_page_reporting=on|off\u0026quot; --fs \u0026lt;fs\u0026gt;... virtio-fs parameters \u0026quot;tag=\u0026lt;tag_name\u0026gt;,socket=\u0026lt;socket_path\u0026gt;,num_queues=\u0026lt;number_of_queues\u0026gt;,queue_size=\u0026lt;size_of_each_queue\u0026gt;,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --pmem \u0026lt;pmem\u0026gt;... Persistent memory parameters \u0026quot;file=\u0026lt;backing_file_path\u0026gt;,size=\u0026lt;persistent_memory_size\u0026gt;,iommu=on|off,discard_writes=on|off,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --serial \u0026lt;serial\u0026gt; Control serial port: off|null|pty|tty|file=\u0026lt;/path/to/a/file\u0026gt;|socket=\u0026lt;/path/to/a/file\u0026gt; [default: null] --console \u0026lt;console\u0026gt; Control (virtio) console: \u0026quot;off|null|pty|tty|file=\u0026lt;/path/to/a/file\u0026gt;,iommu=on|off\u0026quot; [default: tty] --device \u0026lt;device\u0026gt;... Direct device assignment parameters \u0026quot;path=\u0026lt;device_path\u0026gt;,iommu=on|off,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --user-device \u0026lt;user-device\u0026gt;... Userspace device socket=\u0026lt;socket_path\u0026gt;,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --vdpa \u0026lt;vdpa\u0026gt;... vDPA device \u0026quot;path=\u0026lt;device_path\u0026gt;,num_queues=\u0026lt;number_of_queues\u0026gt;,iommu=on|off,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --vsock \u0026lt;vsock\u0026gt; Virtio VSOCK parameters \u0026quot;cid=\u0026lt;context_id\u0026gt;,socket=\u0026lt;socket_path\u0026gt;,iommu=on|off,id=\u0026lt;device_id\u0026gt;,pci_segment=\u0026lt;segment_id\u0026gt;\u0026quot; --pvpanic Enable pvpanic device --numa \u0026lt;numa\u0026gt;... Settings related to a given NUMA node \u0026quot;guest_numa_id=\u0026lt;node_id\u0026gt;,cpus=\u0026lt;cpus_id\u0026gt;,distances=\u0026lt;list_of_distances_to_destination_nodes\u0026gt;,memory_zones=\u0026lt;list_of_memory_zones\u0026gt;,sgx_epc_sections=\u0026lt;list_of_sgx_epc_sections\u0026gt;,pci_segments=\u0026lt;list_of_pci_segments\u0026gt;\u0026quot; --watchdog Enable virtio-watchdog -v... Sets the level of debugging output --log-file \u0026lt;log-file\u0026gt; Log file. Standard error is used if not specified --api-socket \u0026lt;api-socket\u0026gt; HTTP API socket (UNIX domain socket): path=\u0026lt;/path/to/a/file\u0026gt; or fd=\u0026lt;fd\u0026gt;. --event-monitor \u0026lt;event-monitor\u0026gt; File to report events on: path=\u0026lt;/path/to/a/file\u0026gt; or fd=\u0026lt;fd\u0026gt; --restore \u0026lt;restore\u0026gt; Restore from a VM snapshot. Restore parameters \u0026quot;source_url=\u0026lt;source_url\u0026gt;,prefault=on|off\u0026quot; `source_url` should be a valid URL (e.g file:///foo/bar or tcp://192.168.1.10/foo) `prefault` brings memory pages in when enabled (disabled by default) --seccomp \u0026lt;seccomp\u0026gt; [default: true] [possible values: true, false, log] --tpm \u0026lt;tpm\u0026gt; TPM device \u0026quot;(UNIX Domain Socket from swtpm) socket=\u0026lt;/path/to/a/socket\u0026gt;\u0026quot; --sgx-epc \u0026lt;sgx-epc\u0026gt;... SGX EPC parameters \u0026quot;id=\u0026lt;epc_section_identifier\u0026gt;,size=\u0026lt;epc_section_size\u0026gt;,prefault=on|off\u0026quot; -V, --version Print version -h, --help Print help  ch-remote binary # The ch-remote binary that is used for controlling an running Virtual Machine has the following help output (as of v37.0):\nch-remote --help Remotely control a cloud-hypervisor VMM. Usage: ch-remote [OPTIONS] [COMMAND] Commands: add-device Add VFIO device add-disk Add block device add-fs Add virtio-fs backed fs device add-pmem Add persistent memory device add-net Add network device add-user-device Add userspace device add-vdpa Add vDPA device add-vsock Add vsock device remove-device Remove VFIO device info Info on the VM counters Counters from the VM pause Pause the VM reboot Reboot the VM power-button Trigger a power button in the VM resize Resize the VM resize-zone Resize a memory zone resume Resume the VM boot Boot a created VM delete Delete a VM shutdown Shutdown the VM snapshot Create a snapshot from VM restore Restore VM from a snapshot coredump Create a coredump from VM send-migration Initiate a VM migration receive-migration Receive a VM migration create Create VM from a JSON configuration ping Ping the VMM to check for API server availability shutdown-vmm Shutdown the VMM help Print this message or the help of the given subcommand(s) Options: --api-socket \u0026lt;api-socket\u0026gt; HTTP API socket path (UNIX domain socket). -h, --help Print help -V, --version Print version  "}).add({id:3,href:"/docs/prologue/",title:"Prologue",description:"Prologue Doks.",content:""}).add({id:4,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener('input',b,!0);function b(){var b,e;const d=5;b=this.value,e=a.search(b,{limit:d,enrich:!0});const c=new Map;for(const a of e.flatMap(a=>a.result)){if(c.has(a.doc.href))continue;c.set(a.doc.href,a.doc)}if(suggestions.innerHTML="",suggestions.classList.remove('d-none'),c.size===0&&b){const a=document.createElement('div');a.innerHTML=`No results for "<strong>${b}</strong>"`,a.classList.add("suggestion__no-results"),suggestions.appendChild(a);return}for(const[h,g]of c){const b=document.createElement('div');suggestions.appendChild(b);const a=document.createElement('a');a.href=h,b.appendChild(a);const e=document.createElement('span');e.textContent=g.title,e.classList.add("suggestion__title"),a.appendChild(e);const f=document.createElement('span');if(f.textContent=g.description,f.classList.add("suggestion__description"),a.appendChild(f),suggestions.appendChild(b),suggestions.childElementCount==d)break}}})()