var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(c){const d=suggestions.classList.contains('d-none');if(d)return;const a=[...suggestions.querySelectorAll('a')];if(a.length===0)return;const b=a.indexOf(document.activeElement);if(c.key==="ArrowUp"){c.preventDefault();const d=b>0?b-1:0;a[d].focus()}else if(c.key==="ArrowDown"){c.preventDefault();const d=b+1<a.length?b+1:b;a[d].focus()}}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/prologue/introduction/",title:"Introduction",description:"Cloud Hypervisor is an open source Virtual Machine Monitor (VMM) implemented in Rust that focuses on running modern, cloud workloads, with minimal hardware emulation.",content:"Quick Start # See the Quick Start â†’ for a single page guide to getting started with Cloud Hypevisor.\nGo further # Currently the majority of the documentation is in the source tree but a user guide, which will be hosted on this website is planned.\nUser documentation # Here are some documents related to particular features and functionality that a user of Cloud Hypervisor might be interested in:\n HTTP API ARM64 support Virtual CPU configuration Using virtio-fs How to hotplug devices How to use Intel SGX vIOMMU configuration I/O throttling for network and block devices Live migration Using a MACvTAP bridge to place a VM on the host network Advanced memory configuration Snapshot \u0026amp; Restore of a VM Building a UEFI firmware for use with Cloud Hypervisor Using VFIO to passthrough PCI devices Emulated NVMe controller via vfio-user Using virtio-fs as as a root filesystem Running and debugging Windows guests  Developer documentation # These documents are primarily for developers who are working on the project:\n Updating the custom image Debug port logging for performance testing Correct use of logging How to profile Cloud Hypervisor Seccomp filter debugging Testing vhost-user-block Testing vhost-user-net  Contributing # See the CONTRIBUTING file in the source tree for details of how to contribute.\nHelp # Need help with Cloud Hypervisor? Open a discussion thread on Github or ask in our Slack.\n"}).add({id:1,href:"/docs/prologue/quick-start/",title:"Quick Start",description:"One page summary of how to get started with Cloud Hypervisor",content:"Setup # We create a folder to build and run cloud-hypervisor at $HOME/cloud-hypervisor\nexport CLOUDH=$HOME/cloud-hypervisor # Add this to .bashrc so it is available for subsequent bash sessions echo \u0026quot;export CLOUDH=$HOME/cloud-hypervisor\u0026quot; \u0026gt;\u0026gt; ~/.bashrc mkdir $CLOUDH  Install prerequisites # You need to install some prerequisite packages in order to build and test Cloud Hypervisor. Here, all the steps are based on Ubuntu, for other Linux distributions please replace the package manager and package name.\n# Install git sudo apt install git # Install rust tool chain curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # Install build-essential sudo apt install build-essential # Install qemu apt install qemu qemu-utils  You will need to restart bash to get rustup into your path, or add it manually.\n# If you want to build statically linked binary please add musl target apt install musl musl-tools rustup target add x86_64-unknown-linux-musl  Clone and build # First you need to clone and build the cloud-hypervisor repo:\npushd $CLOUDH git clone https://github.com/cloud-hypervisor/cloud-hypervisor.git cd cloud-hypervisor cargo build --release # We need to give the cloud-hypervisor binary the NET_ADMIN capabilities for it to set TAP interfaces up on the host. sudo setcap cap_net_admin+ep ./target/release/cloud-hypervisor # If you want to build statically linked binary cargo build --release --target=x86_64-unknown-linux-musl --all popd  This will build a cloud-hypervisor binary under $CLOUDH/cloud-hypervisor/target/release/cloud-hypervisor.\nContainerized builds and tests # If you want to build and test Cloud Hypervisor without having to install all the required dependencies (The rust toolchain, cargo tools, etc), you can also use Cloud Hypervisor\u0026rsquo;s development script: dev_cli.sh. Please note that upon its first invocation, this script will pull a fairly large container image.\nFor example, to build the Cloud Hypervisor release binary:\npushd $CLOUDH cd cloud-hypervisor ./scripts/dev_cli.sh build --release  With dev_cli.sh, one can also run the Cloud Hypervisor CI locally. This can be very convenient for debugging CI errors without having to fully rely on the Cloud Hypervisor CI infrastructure.\nFor example, to run the Cloud Hypervisor unit tests:\n./scripts/dev_cli.sh tests --unit  Run the ./scripts/dev_cli.sh --help command to view all the supported development script commands and their related options.\nRun # You can run a guest VM by either using an existing cloud image or booting into your own kernel and disk image.\nCloud image # Cloud Hypervisor supports booting disk images containing all needed components to run cloud workloads, a.k.a. cloud images. To do that we rely on the Rust Hypervisor Firmware project to provide an ELF formatted KVM firmware for cloud-hypervisor to directly boot into.\nWe need to get the latest rust-hypervisor-firmware release and also a working cloud image. Here we will use a Ubuntu image:\npushd $CLOUDH wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw wget https://github.com/cloud-hypervisor/rust-hypervisor-firmware/releases/download/0.3.2/hypervisor-fw popd  pushd $CLOUDH sudo setcap cap_net_admin+ep ./cloud-hypervisor/target/release/cloud-hypervisor ./cloud-hypervisor/target/release/cloud-hypervisor \\ --kernel ./hypervisor-fw \\ --disk path=focal-server-cloudimg-amd64.raw \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot; popd  Multiple arguments can be given to the --disk parameter.\nCustom kernel and disk image # Building your kernel # Cloud Hypervisor also supports direct kernel boot into a vmlinux ELF kernel. In order to support virtio-iommu we have our own development branch. You are of course able to use your own kernel but these instructions will continue with the version that we develop and test against.\nTo build the kernel:\n# Clone the Cloud Hypervisor Linux branch pushd $CLOUDH git clone --depth 1 https://github.com/cloud-hypervisor/linux.git -b ch-5.14 linux-cloud-hypervisor pushd linux-cloud-hypervisor # Use the cloud-hypervisor kernel config to build your kernel cp $CLOUDH/cloud-hypervisor/resources/linux-config-x86_64 .config make bzImage -j `nproc` popd  The vmlinux kernel image will then be located at linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin.\nDisk image # For the disk image, we will use a Ubuntu cloud image that contains a root partition:\npushd $CLOUDH wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw popd  Booting the guest VM # Now we can directly boot into our custom kernel and make it use the Ubuntu root partition. If we want to have 4 vCPUs and 1024 MBytes of memory:\npushd $CLOUDH sudo setcap cap_net_admin+ep ./cloud-hypervisor/target/release/cloud-hypervisor ./cloud-hypervisor/target/release/cloud-hypervisor \\ --kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \\ --disk path=focal-server-cloudimg-amd64.raw \\ --cmdline \u0026quot;console=hvc0 root=/dev/vda1 rw\u0026quot; \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;  The above example use the virtio-console device as the guest console, and this device may not be enabled soon enough by the guest kernel to get early kernel debug messages.\nWhen in need for earlier debug messages, using the legacy serial device based console is preferred:\n./cloud-hypervisor/target/release/cloud-hypervisor \\ --kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \\ --console off \\ --serial tty \\ --disk path=focal-server-cloudimg-amd64.raw \\ --cmdline \u0026quot;console=ttyS0 root=/dev/vda1 rw\u0026quot; \\ --cpus boot=4 \\ --memory size=1024M \\ --net \u0026quot;tap=,mac=,ip=,mask=\u0026quot;  "}).add({id:2,href:"/docs/prologue/commands/",title:"Commands",description:"Cloud Hypervisor is composed of multiple binaries. This document shows what they are and their help output.",content:"cloud-hypervisor binary # The main cloud-hypervisor has the following help output (as of v18.0):\ncloud-hypervisor --help cloud-hypervisor v18.0 The Cloud Hypervisor Authors Launch a cloud-hypervisor VMM. USAGE: cloud-hypervisor [FLAGS] [OPTIONS] FLAGS: -h, --help Prints help information -v Sets the level of debugging output -V, --version Prints version information --watchdog Enable virtio-watchdog OPTIONS: --api-socket \u0026lt;api-socket\u0026gt; HTTP API socket (UNIX domain socket): path=\u0026lt;/path/to/a/file\u0026gt; or fd=\u0026lt;fd\u0026gt;. --balloon \u0026lt;balloon\u0026gt; Balloon parameters \u0026quot;size=\u0026lt;balloon_size\u0026gt;,deflate_on_oom=on|off\u0026quot; --cmdline \u0026lt;cmdline\u0026gt; Kernel command line --console \u0026lt;console\u0026gt; Control (virtio) console: \u0026quot;off|null|pty|tty|file=/path/to/a/file,iommu=on|off\u0026quot; [default: tty] --cpus \u0026lt;cpus\u0026gt; boot=\u0026lt;boot_vcpus\u0026gt;,max=\u0026lt;max_vcpus\u0026gt;,topology=\u0026lt;threads_per_core\u0026gt;:\u0026lt;cores_per_die\u0026gt;:\u0026lt;dies_per_package\u0026gt;:\u0026lt;packages\u0026gt;,kvm_hyperv=on|off,max_phys_bits=\u0026lt;maximum_number_of_physical_bits\u0026gt; [default: boot=1] --device \u0026lt;device\u0026gt; Direct device assignment parameters \u0026quot;path=\u0026lt;device_path\u0026gt;,iommu=on|off,id=\u0026lt;device_id\u0026gt;\u0026quot; --disk \u0026lt;disk\u0026gt; Disk parameters \u0026quot;path=\u0026lt;disk_image_path\u0026gt;,readonly=on|off,direct=on|off,iommu=on|off,num_queues=\u0026lt;number_of_queues\u0026gt;,queue_size=\u0026lt;size_of_each_queue\u0026gt;,vhost_user=on|off,socket=\u0026lt;vhost_user_socket_path\u0026gt;,poll_queue=on|off,bw_size=\u0026lt;bytes\u0026gt;,bw_one_time_burst=\u0026lt;bytes\u0026gt;,bw_refill_time=\u0026lt;ms\u0026gt;,ops_size=\u0026lt;io_ops\u0026gt;,ops_one_time_burst=\u0026lt;io_ops\u0026gt;,ops_refill_time=\u0026lt;ms\u0026gt;,id=\u0026lt;device_id\u0026gt;\u0026quot; --event-monitor \u0026lt;event-monitor\u0026gt; File to report events on: path=\u0026lt;/path/to/a/file\u0026gt; or fd=\u0026lt;fd\u0026gt; --fs \u0026lt;fs\u0026gt; virtio-fs parameters \u0026quot;tag=\u0026lt;tag_name\u0026gt;,socket=\u0026lt;socket_path\u0026gt;,num_queues=\u0026lt;number_of_queues\u0026gt;,queue_size=\u0026lt;size_of_each_queue\u0026gt;,dax=on|off,cache_size=\u0026lt;DAX cache size: default 8Gib\u0026gt;,id=\u0026lt;device_id\u0026gt;\u0026quot; --initramfs \u0026lt;initramfs\u0026gt; Path to initramfs image --kernel \u0026lt;kernel\u0026gt; Path to loaded kernel. This may be a kernel or firmware that supports a PVH entry point (e.g. vmlinux) or architecture equivalent --log-file \u0026lt;log-file\u0026gt; Log file. Standard error is used if not specified --memory \u0026lt;memory\u0026gt; Memory parameters \u0026quot;size=\u0026lt;guest_memory_size\u0026gt;,mergeable=on|off,shared=on|off,hugepages=on|off,hugepage_size=\u0026lt;hugepage_size\u0026gt;,hotplug_method=acpi|virtio-mem,hotplug_size=\u0026lt;hotpluggable_memory_size\u0026gt;,hotplugged_size=\u0026lt;hotplugged_memory_size\u0026gt;\u0026quot; [default: size=512M] --memory-zone \u0026lt;memory-zone\u0026gt; User defined memory zone parameters \u0026quot;size=\u0026lt;guest_memory_region_size\u0026gt;,file=\u0026lt;backing_file\u0026gt;,shared=on|off,hugepages=on|off,hugepage_size=\u0026lt;hugepage_size\u0026gt;,host_numa_node=\u0026lt;node_id\u0026gt;,id=\u0026lt;zone_identifier\u0026gt;,hotplug_size=\u0026lt;hotpluggable_memory_size\u0026gt;,hotplugged_size=\u0026lt;hotplugged_memory_size\u0026gt;\u0026quot; --net \u0026lt;net\u0026gt; Network parameters \u0026quot;tap=\u0026lt;if_name\u0026gt;,ip=\u0026lt;ip_addr\u0026gt;,mask=\u0026lt;net_mask\u0026gt;,mac=\u0026lt;mac_addr\u0026gt;,fd=\u0026lt;fd1:fd2...\u0026gt;,iommu=on|off,num_queues=\u0026lt;number_of_queues\u0026gt;,queue_size=\u0026lt;size_of_each_queue\u0026gt;,id=\u0026lt;device_id\u0026gt;,vhost_user=\u0026lt;vhost_user_enable\u0026gt;,socket=\u0026lt;vhost_user_socket_path\u0026gt;,vhost_mode=client|server,bw_size=\u0026lt;bytes\u0026gt;,bw_one_time_burst=\u0026lt;bytes\u0026gt;,bw_refill_time=\u0026lt;ms\u0026gt;,ops_size=\u0026lt;io_ops\u0026gt;,ops_one_time_burst=\u0026lt;io_ops\u0026gt;,ops_refill_time=\u0026lt;ms\u0026gt;\u0026quot; --numa \u0026lt;numa\u0026gt; Settings related to a given NUMA node \u0026quot;guest_numa_id=\u0026lt;node_id\u0026gt;,cpus=\u0026lt;cpus_id\u0026gt;,distances=\u0026lt;list_of_distances_to_destination_nodes\u0026gt;,memory_zones=\u0026lt;list_of_memory_zones\u0026gt;,sgx_epc_sections=\u0026lt;list_of_sgx_epc_sections\u0026gt;\u0026quot; --pmem \u0026lt;pmem\u0026gt; Persistent memory parameters \u0026quot;file=\u0026lt;backing_file_path\u0026gt;,size=\u0026lt;persistent_memory_size\u0026gt;,iommu=on|off,mergeable=on|off,discard_writes=on|off,id=\u0026lt;device_id\u0026gt;\u0026quot; --restore \u0026lt;restore\u0026gt; Restore from a VM snapshot. Restore parameters \u0026quot;source_url=\u0026lt;source_url\u0026gt;,prefault=on|off\u0026quot; `source_url` should be a valid URL (e.g file:///foo/bar or tcp://192.168.1.10/foo) `prefault` brings memory pages in when enabled (disabled by default) --rng \u0026lt;rng\u0026gt; Random number generator parameters \u0026quot;src=\u0026lt;entropy_source_path\u0026gt;,iommu=on|off\u0026quot; [default: src=/dev/urandom] --seccomp \u0026lt;seccomp\u0026gt; [default: true] [possible values: true, false, log] --serial \u0026lt;serial\u0026gt; Control serial port: off|null|pty|tty|file=/path/to/a/file [default: null] --sgx-epc \u0026lt;sgx-epc\u0026gt; SGX EPC parameters \u0026quot;id=\u0026lt;epc_section_identifier\u0026gt;,size=\u0026lt;epc_section_size\u0026gt;,prefault=on|off\u0026quot; --user-device \u0026lt;user-device\u0026gt; Userspace device socket=\u0026lt;socket_path\u0026gt;,id=\u0026lt;device_id\u0026gt;\u0026quot; --vsock \u0026lt;vsock\u0026gt; Virtio VSOCK parameters \u0026quot;cid=\u0026lt;context_id\u0026gt;,socket=\u0026lt;socket_path\u0026gt;,iommu=on|off,id=\u0026lt;device_id\u0026gt;\u0026quot;  ch-remote binary # The ch-remote binary that is used for controlling an running Virtual Machine has the following help output (as of v18.0):\nch-remote --help ch-remote The Cloud Hypervisor Authors Remotely control a cloud-hypervisor VMM. USAGE: ch-remote --api-socket \u0026lt;api-socket\u0026gt; \u0026lt;SUBCOMMAND\u0026gt; FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: --api-socket \u0026lt;api-socket\u0026gt; HTTP API socket path (UNIX domain socket). SUBCOMMANDS: add-device Add VFIO device add-disk Add block device add-fs Add virtio-fs backed fs device add-net Add network device add-pmem Add persistent memory device add-user-device Add userspace device add-vsock Add vsock device counters Counters from the VM help Prints this message or the help of the given subcommand(s) info Info on the VM pause Pause the VM power-button Trigger a power button in the VM reboot Reboot the VM receive-migration Receive a VM migration remove-device Remove VFIO device resize Resize the VM resize-zone Resize a memory zone restore Restore VM from a snapshot resume Resume the VM send-migration Initiate a VM migration shutdown Shutdown the VM snapshot Create a snapshot from VM  "}).add({id:3,href:"/docs/prologue/",title:"Prologue",description:"Prologue Doks.",content:""}).add({id:4,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener('input',b,!0);function b(){var b,e;const d=5;b=this.value,e=a.search(b,{limit:d,enrich:!0});const c=new Map;for(const a of e.flatMap(a=>a.result)){if(c.has(a.doc.href))continue;c.set(a.doc.href,a.doc)}if(suggestions.innerHTML="",suggestions.classList.remove('d-none'),c.size===0&&b){const a=document.createElement('div');a.innerHTML=`No results for "<strong>${b}</strong>"`,a.classList.add("suggestion__no-results"),suggestions.appendChild(a);return}for(const[h,g]of c){const b=document.createElement('div');suggestions.appendChild(b);const a=document.createElement('a');a.href=h,b.appendChild(a);const e=document.createElement('span');e.textContent=g.title,e.classList.add("suggestion__title"),a.appendChild(e);const f=document.createElement('span');if(f.textContent=g.description,f.classList.add("suggestion__description"),a.appendChild(f),suggestions.appendChild(b),suggestions.childElementCount==d)break}}})()